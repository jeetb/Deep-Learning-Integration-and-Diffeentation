{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d0fc470a746fcf07f76d09b0ee0cea46ebcfc4e"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "fcd7da31-a545-41a8-ae89-376b2a8f25e8",
    "_uuid": "66c19897a47dc4565c6c24ed29d4dccada1f0d71"
   },
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 29976  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'prim_fwd/minialpha.train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dad6ce5dfcb0935bf9722da247b50efacb11676"
   },
   "source": [
    "## The data\n",
    "The data is present in a tab delimited CSV file. To give it a quick overview we will load it with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6c6891c41f2dfcd07554e5fc13eb7410dc432b4a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "559cc4eb61559b022b732b38e3bbc6d8719fecd1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BfkEwEd2</th>\n",
       "      <th>kurd1d2wEd2urd1d3wEd3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BfwEd3</td>\n",
       "      <td>urd1dawEda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BfkEwEda</td>\n",
       "      <td>kurd1d2wEd2urd1dawEda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bfkud2EsE</td>\n",
       "      <td>kwEd2sE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BfkEwEe1</td>\n",
       "      <td>kurd1d2wEd2tE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BfkEuEyE</td>\n",
       "      <td>kurd1d2wEd2kue1uEpEyE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    BfkEwEd2  kurd1d2wEd2urd1d3wEd3\n",
       "0     BfwEd3             urd1dawEda\n",
       "1   BfkEwEda  kurd1d2wEd2urd1dawEda\n",
       "2  Bfkud2EsE                kwEd2sE\n",
       "3   BfkEwEe1          kurd1d2wEd2tE\n",
       "4   BfkEuEyE  kurd1d2wEd2kue1uEpEyE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b4402602262a7d854cbb664985e4ea0377083b9e"
   },
   "outputs": [],
   "source": [
    "del df # Save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba16c99cab41ed4b7e2ca43da18e77ada52fd6dd"
   },
   "source": [
    "## Vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "ea0e2e7f-f65e-4fe6-b75e-304efb56f04f",
    "_uuid": "06a6c35f13c93a188005227a3b98fc9b7b21a201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 29976\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "# Loop over lines\n",
    "lines = open(data_path).read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    # Input and target are split by tabs\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # Create a set of all unique characters in the input\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "            \n",
    "    # Create a set of all unique output characters\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "print('Number of samples:', len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "de998ecc-341a-4a47-830a-1dc20f0ae904",
    "_uuid": "555c970d2117022365512bf3d84246a909f44fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique input tokens: 32\n",
      "Number of unique output tokens: 34\n"
     ]
    }
   ],
   "source": [
    "input_characters = sorted(list(input_characters)) # Make sure we achieve the same order in our input chars\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters) \n",
    "num_decoder_tokens = len(target_characters) \n",
    "\n",
    "\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b99e23e7-2faa-4ce7-a6f3-784e02ef46c7",
    "_uuid": "a52af2a93fd956dd9a33c02baf3aa5a73bd7a1ec"
   },
   "outputs": [],
   "source": [
    "# This works very similar to a tokenizer\n",
    "# The index maps a character to a number\n",
    "input_token_index = {char: i for i, char in enumerate(input_characters)}\n",
    "target_token_index = {char: i for i, char in enumerate(target_characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "76afcd7b-23bd-44c3-b3fb-978cba081f21",
    "_uuid": "43f7153aa4595b48da4338c300712a7c16fcfa01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "7d4ad6ea-17fc-479a-87fe-3822fdd161c2",
    "_uuid": "b88478d0f15b9642f905395d8c45445f628719b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length for inputs: 74\n",
      "Max sequence length for outputs: 508\n"
     ]
    }
   ],
   "source": [
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # Get longest sequences length\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "2c39c2c1-072c-4e5c-99bd-e13a62ebed04",
    "_uuid": "7231966fbdb13bffb3805d69856d354acb1d15ce"
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data is the same as decoder_input_data but offset by one timestep. \n",
    "# decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "8d1014c8-82a6-4988-a31e-33bd3498b585",
    "_uuid": "d12785864708eaa8feec3b7aba862b02bec76e3b"
   },
   "outputs": [],
   "source": [
    "# Loop over input texts\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    # Loop over each char in an input text\n",
    "    for t, char in enumerate(input_text):\n",
    "        # Create one hot encoding by setting the index to 1\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    # Loop over each char in the output text\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "909d7eb1-54f5-4b96-8745-34080104dcc4",
    "_uuid": "14f0c33f929a0ef7aaf1fa23cff824ecb4b4fe6f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name = 'encoder_inputs')\n",
    "\n",
    "# The return_state contructor argument, configuring a RNN layer to return a list \n",
    "# where the first entry is the outputs and the next entries are the internal RNN states. \n",
    "# This is used to recover the states of the encoder.\n",
    "encoder = LSTM(latent_dim, \n",
    "                    return_state=True, \n",
    "                    name = 'encoder')\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), \n",
    "                       name = 'decoder_inputs')\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, \n",
    "                         return_sequences=True, \n",
    "                         return_state=True, \n",
    "                         name = 'decoder_lstm')\n",
    "\n",
    "# The inital_state call argument, specifying the initial state(s) of a RNN. \n",
    "# This is used to pass the encoder states to the decoder as initial states.\n",
    "# Basically making the first memory of the decoder the encoded semantics\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, \n",
    "                      activation='softmax', \n",
    "                      name = 'decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "1e70cde9-4c5d-4de2-8f41-90a7c5bd3475",
    "_kg_hide-output": true,
    "_uuid": "72492095eea44a02d9f392b2274606382cf11f64",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "375/375 [==============================] - 1983s 5s/step - loss: 0.2052 - val_loss: 0.1270\n",
      "Epoch 2/5\n",
      "375/375 [==============================] - 1886s 5s/step - loss: 0.1813 - val_loss: 0.1003\n",
      "Epoch 3/5\n",
      "375/375 [==============================] - 2145s 6s/step - loss: 0.1552 - val_loss: 0.0888\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 3068s 8s/step - loss: 0.1415 - val_loss: 0.0817\n",
      "Epoch 5/5\n",
      "375/375 [==============================] - 2031s 5s/step - loss: 0.1284 - val_loss: 0.0756\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "history = model.fit([encoder_input_data, decoder_input_data], \n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)\n",
    "# Save model\n",
    "#model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5979b64e11875088255f5d91c9665a4418a46b08"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55d758b0-b642-4974-b028-82f920d4602b",
    "_uuid": "ca7c6f484ebbe4aed426744aa2197cf194c3ba26"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2ad8f7e4-72a4-4884-8be4-d9fb4e1bd126",
    "_uuid": "61c95ebe8356e8f91cd8affc10e14acc1596421d"
   },
   "source": [
    "# Creating inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "239a7196-6485-4f65-8dea-4ca1a9aa87df",
    "_uuid": "1c10010e6db47a30edd82bee80e77a0241a79fcf"
   },
   "outputs": [],
   "source": [
    "# Define encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "bb95b6a7-772b-458a-bd23-411e98e32205",
    "_uuid": "343f0049794312cf4fd1511c5f34ce36a9e26982"
   },
   "outputs": [],
   "source": [
    "# Define decoder model\n",
    "\n",
    "# Inputs from the encoder\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "# Create a combined memory to input into the decoder\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "# Predict next char\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# The model takes in the encoder memory plus it's own memory as an input and spits out \n",
    "# a prediction plus its own memory to be used for the next char\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "5f19066a-c7e4-421b-a6cf-a2f463fd75fb",
    "_uuid": "c3c1dd185d7e8532f088903bf634f6c14d18fb2d"
   },
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = {i: char \n",
    "                            for char, i in input_token_index.items()}\n",
    "reverse_target_char_index = {i: char \n",
    "                             for char, i in target_token_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d6e067d6-e27f-47e9-b36d-8c8469fef085",
    "_uuid": "a1b14b5e6f0a1c34d6ffd3a27b302e79c0d31ea5"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    # Loop untill we recieve a stop sign\n",
    "    while not stop_condition:\n",
    "        # Get output and internal states of the decoder \n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Get the predicted token (the token with the highest score)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # Get the character belonging to the token\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        # Append char to output\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96c02bf34edb09a51cefa1e34ed3425e96fd69eb"
   },
   "source": [
    "## Giving it a spin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "c171e7a9-5642-43b8-ac4d-a0150ad20e14",
    "_uuid": "3a78fc8d64f0bca2650fd73c6356fd0b5a6bf171"
   },
   "outputs": [],
   "source": [
    "my_text = 'BfkEob'\n",
    "placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "e211bbb7-1dc7-40aa-96b9-d9372adea403",
    "_uuid": "10d39df3fdd008e15853d873c4a496a0568a872a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B 5\n",
      "1 f 13\n",
      "2 k 17\n",
      "3 E 8\n",
      "4 o 21\n",
      "5 b 10\n"
     ]
    }
   ],
   "source": [
    "for i, char in enumerate(my_text):\n",
    "    print(i,char, input_token_index[char])\n",
    "    placeholder[0,i,input_token_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_cell_guid": "9c45d63b-fcec-4611-bc76-59b1c5322eb9",
    "_uuid": "d94521498349ca4e3a8010f283f9e4057a5b8e78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ure1dawEda\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(placeholder) #correct answer is kurd1d2wEd2uEob so kinda close but not exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f27b5be-c657-4b6e-8484-d53615f3fab8",
    "_uuid": "510249e948055ee16fe81474b22eafd3055bd310",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decode_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_uuid": "a7236af49fd8aaf72db105ec3d482687f4ae297a"
   },
   "outputs": [],
   "source": [
    "my_text = 'BfuEkeaE'\n",
    "placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 B 5\n",
      "1 f 13\n",
      "2 u 27\n",
      "3 E 8\n",
      "4 k 17\n",
      "5 e 12\n",
      "6 a 9\n",
      "7 E 8\n"
     ]
    }
   ],
   "source": [
    "for i, char in enumerate(my_text):\n",
    "    print(i,char, input_token_index[char])\n",
    "    placeholder[0,i,input_token_index[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kure1dawEdaurd1dawEda\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sequence(placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettoword(let):\n",
    "    my_dict = {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9','4':'a', 'E': 'b', 'I': 'c', '(+)': 'd', '(-)': 'e', \"Y'\": 'f', 'abs': 'g', 'acos': 'h', 'acosh': 'i', '+': 'k', 'asin': 'l', 'asinh': 'm', 'atan': 'n', 'atanh': 'o', 'cos': 'p', 'cosh': 'q', '/': 'r', 'exp': 's', 'ln': 't', '*': 'u', 'pi': 'v', 'pow': 'w', 'sign': 'x', 'sin': 'y', 'sinh': 'z', 'sqrt': 'A', '-': 'B', 'tan': 'C', 'tanh': 'D', 'x': 'E'}\n",
    "    rev = {v:k for k, v in my_dict.items()}\n",
    "    let = list(let)\n",
    "    ret = []\n",
    "    for key in let:\n",
    "        ret.append(rev[key])\n",
    "    return \" \".join(ret)\n",
    "        \n",
    "def wordtolet(word):\n",
    "    my_dict = {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', '4':'a', 'E': 'b', 'I': 'c', '(+)': 'd', '(-)': 'e', \"Y'\": 'f', 'abs': 'g', 'acos': 'h', 'acosh': 'i', '+': 'k', 'asin': 'l', 'asinh': 'm', 'atan': 'n', 'atanh': 'o', 'cos': 'p', 'cosh': 'q', '/': 'r', 'exp': 's', 'ln': 't', '*': 'u', 'pi': 'v', 'pow': 'w', 'sign': 'x', 'sin': 'y', 'sinh': 'z', 'sqrt': 'A', '-': 'B', 'tan': 'C', 'tanh': 'D', 'x': 'E'}\n",
    "    word = word.split()\n",
    "    ret = []\n",
    "    for key in word:\n",
    "        ret.append(my_dict[key])\n",
    "    return \"\".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(text):\n",
    "    my_text = wordtolet(text)\n",
    "    placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))\n",
    "    for i, char in enumerate(my_text):\n",
    "        #print(i,char, input_token_index[char])\n",
    "        placeholder[0,i,input_token_index[char]] = 1\n",
    "    sol = decode_sequence(placeholder)[:-1]\n",
    "    sol = lettoword(sol)\n",
    "    return sol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy():\n",
    "    test_lines = open('alpha_integ.test').read().split('\\n')\n",
    "    \n",
    "        \n",
    "    tot = len(input_texts)\n",
    "    corr = 0\n",
    "    for test_line in test_lines:\n",
    "        test_input_eq, test_target_eq = test_line.split('\\t')\n",
    "        placeholder = np.zeros((1,len(test_input_eq)+10,num_encoder_tokens))\n",
    "        for i, char in enumerate(test_input_eq):\n",
    "            #print(i,char, input_token_index[char])\n",
    "            placeholder[0,i,input_token_index[char]] = 1\n",
    "        if(decode_sequence(placeholder)[:-1] == test_target_eq):\n",
    "            print(test_input_eq, test_target_eq)\n",
    "            corr += 1\n",
    "    return (corr/tot)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples():\n",
    "    train_lines = open('prim_fwd/minialpha.train').read().split('\\n')\n",
    "    for train_line in train_lines[:1]:\n",
    "        train_input_eq, train_target_eq = train_line.split('\\t')\n",
    "        train_input_eq = lettoword(train_input_eq)\n",
    "        train_target_eq = lettoword(train_target_eq)\n",
    "        sol = solve(train_input_eq)\n",
    "        #if(solve(train_input_eq) == train_target_eq):\n",
    "        print(\"\\nInput: \" + train_input_eq + \"\\nPrediction: \" + lettoword(wordtolet(sol)[:min(len(wordtolet(sol)), len(wordtolet(train_target_eq)))]) + \"\\nTarget: \" + train_target_eq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: - Y' + x pow x (+) 2\n",
      "Prediction: + * / (-) 1 (+) 4 pow x (+) 4 * / (+) 1 (+) 4 pow x (+) 4\n",
      "Target: + * / (+) 1 (+) 2 pow x (+) 2 * / (+) 1 (+) 3 pow x (+) 3\n",
      "\n",
      "Input: - Y' pow x (+) 3\n",
      "Prediction: * / (-) 1 (+) 4 pow x (+) 4\n",
      "Target: * / (+) 1 (+) 4 pow x (+) 4\n",
      "\n",
      "Input: - Y' + x pow x (+) 4\n",
      "Prediction: + * / (-) 1 (+) 4 pow x (+) 4 * / (+) 1 (+) 4 pow x (+) 4\n",
      "Target: + * / (+) 1 (+) 2 pow x (+) 2 * / (+) 1 (+) 4 pow x (+) 4\n"
     ]
    }
   ],
   "source": [
    "print_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: - Y' + x pow x (+) 2\n",
      "Prediction: + * / (+) 1 (+) 2 pow x (+) 2 + * / (+) 1 (+) 4 pow x (+)\n",
      "Target: + * / (+) 1 (+) 2 pow x (+) 2 * / (+) 1 (+) 3 pow x (+) 3\n"
     ]
    }
   ],
   "source": [
    "print_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: int_enc.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: int_enc.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "encoder_model.save('int_enc.tf', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: int_dec.tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: int_dec.tf\\assets\n"
     ]
    }
   ],
   "source": [
    "decoder_model.save('int_dec.tf', save_format = 'tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
