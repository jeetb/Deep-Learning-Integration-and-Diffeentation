{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decoder_tokens  = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_index = {'\\t': 0,\n",
    " '\\n': 1,\n",
    " '0': 2,\n",
    " '1': 3,\n",
    " '2': 4,\n",
    " '3': 5,\n",
    " 'A': 6,\n",
    " 'C': 7,\n",
    " 'D': 8,\n",
    " 'E': 9,\n",
    " 'a': 10,\n",
    " 'b': 11,\n",
    " 'c': 12,\n",
    " 'd': 13,\n",
    " 'e': 14,\n",
    " 'g': 15,\n",
    " 'h': 16,\n",
    " 'i': 17,\n",
    " 'j': 18,\n",
    " 'k': 19,\n",
    " 'l': 20,\n",
    " 'm': 21,\n",
    " 'n': 22,\n",
    " 'o': 23,\n",
    " 'p': 24,\n",
    " 'q': 25,\n",
    " 'r': 26,\n",
    " 's': 27,\n",
    " 't': 28,\n",
    " 'u': 29,\n",
    " 'v': 30,\n",
    " 'w': 31,\n",
    " 'y': 32,\n",
    " 'z': 33}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = {'0': 0,\n",
    " '1': 1,\n",
    " '2': 2,\n",
    " '3': 3,\n",
    " 'A': 4,\n",
    " 'B': 5,\n",
    " 'C': 6,\n",
    " 'D': 7,\n",
    " 'E': 8,\n",
    " 'a': 9,\n",
    " 'b': 10,\n",
    " 'd': 11,\n",
    " 'e': 12,\n",
    " 'f': 13,\n",
    " 'g': 14,\n",
    " 'h': 15,\n",
    " 'i': 16,\n",
    " 'k': 17,\n",
    " 'l': 18,\n",
    " 'm': 19,\n",
    " 'n': 20,\n",
    " 'o': 21,\n",
    " 'p': 22,\n",
    " 'q': 23,\n",
    " 'r': 24,\n",
    " 's': 25,\n",
    " 't': 26,\n",
    " 'u': 27,\n",
    " 'v': 28,\n",
    " 'w': 29,\n",
    " 'y': 30,\n",
    " 'z': 31}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_char_index = {0: '\\t',\n",
    " 1: '\\n',\n",
    " 2: '0',\n",
    " 3: '1',\n",
    " 4: '2',\n",
    " 5: '3',\n",
    " 6: 'A',\n",
    " 7: 'C',\n",
    " 8: 'D',\n",
    " 9: 'E',\n",
    " 10: 'a',\n",
    " 11: 'b',\n",
    " 12: 'c',\n",
    " 13: 'd',\n",
    " 14: 'e',\n",
    " 15: 'g',\n",
    " 16: 'h',\n",
    " 17: 'i',\n",
    " 18: 'j',\n",
    " 19: 'k',\n",
    " 20: 'l',\n",
    " 21: 'm',\n",
    " 22: 'n',\n",
    " 23: 'o',\n",
    " 24: 'p',\n",
    " 25: 'q',\n",
    " 26: 'r',\n",
    " 27: 's',\n",
    " 28: 't',\n",
    " 29: 'u',\n",
    " 30: 'v',\n",
    " 31: 'w',\n",
    " 32: 'y',\n",
    " 33: 'z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    # Loop untill we recieve a stop sign\n",
    "    while not stop_condition:\n",
    "        # Get output and internal states of the decoder \n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Get the predicted token (the token with the highest score)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # Get the character belonging to the token\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        # Append char to output\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lettoword(let):\n",
    "    my_dict = {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9','4':'a', 'E': 'b', 'I': 'c', '(+)': 'd', '(-)': 'e', \"Y'\": 'f', 'abs': 'g', 'acos': 'h', 'acosh': 'i', '+': 'k', 'asin': 'l', 'asinh': 'm', 'atan': 'n', 'atanh': 'o', 'cos': 'p', 'cosh': 'q', '/': 'r', 'exp': 's', 'ln': 't', '*': 'u', 'pi': 'v', 'pow': 'w', 'sign': 'x', 'sin': 'y', 'sinh': 'z', 'sqrt': 'A', '-': 'B', 'tan': 'C', 'tanh': 'D', 'x': 'E'}\n",
    "    rev = {v:k for k, v in my_dict.items()}\n",
    "    let = list(let)\n",
    "    ret = []\n",
    "    for key in let:\n",
    "        ret.append(rev[key])\n",
    "    return \" \".join(ret)\n",
    "        \n",
    "def wordtolet(word):\n",
    "    my_dict = {'0': '0', '1': '1', '2': '2', '3': '3', '4': '4', '5': '5', '6': '6', '7': '7', '8': '8', '9': '9', '4':'a', 'E': 'b', 'I': 'c', '(+)': 'd', '(-)': 'e', \"Y'\": 'f', 'abs': 'g', 'acos': 'h', 'acosh': 'i', '+': 'k', 'asin': 'l', 'asinh': 'm', 'atan': 'n', 'atanh': 'o', 'cos': 'p', 'cosh': 'q', '/': 'r', 'exp': 's', 'ln': 't', '*': 'u', 'pi': 'v', 'pow': 'w', 'sign': 'x', 'sin': 'y', 'sinh': 'z', 'sqrt': 'A', '-': 'B', 'tan': 'C', 'tanh': 'D', 'x': 'E'}\n",
    "    word = word.split()\n",
    "    ret = []\n",
    "    for key in word:\n",
    "        ret.append(my_dict[key])\n",
    "    return \"\".join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(text):\n",
    "    my_text = wordtolet(text)\n",
    "    placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))\n",
    "    for i, char in enumerate(my_text):\n",
    "        #print(i,char, input_token_index[char])\n",
    "        placeholder[0,i,input_token_index[char]] = 1\n",
    "    sol = decode_sequence(placeholder)[:-1]\n",
    "    sol = lettoword(sol)\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_examples():\n",
    "    train_lines = open('minialpha.train').read().split('\\n')\n",
    "    for train_line in train_lines[:1]:\n",
    "        train_input_eq, train_target_eq = train_line.split('\\t')\n",
    "        train_input_eq = lettoword(train_input_eq)\n",
    "        train_target_eq = lettoword(train_target_eq)\n",
    "        sol = solve(train_input_eq)\n",
    "        #if(solve(train_input_eq) == train_target_eq):\n",
    "        print(\"\\nInput: \" + train_input_eq + \"\\nPrediction: \" + lettoword(wordtolet(sol)[:min(len(wordtolet(sol)), len(wordtolet(train_target_eq)))]) + \"\\nTarget: \" + train_target_eq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "encoder_model = load_model('int_enc.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "decoder_model = load_model('int_dec.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: - Y' + x pow x (+) 2\n",
      "Prediction: + * / (+) 1 (+) 2 pow x (+) 2 + * / (+) 1 (+) 4 pow x (+)\n",
      "Target: + * / (+) 1 (+) 2 pow x (+) 2 * / (+) 1 (+) 3 pow x (+) 3\n"
     ]
    }
   ],
   "source": [
    "print_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
